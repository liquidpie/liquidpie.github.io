<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Vivek Jaiswal</title>
  
  <subtitle>Polyglot Developer | Machine Learning Enthusiast | Aesthete | Tripper | Fitness Lover</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://liquidpie.github.io/"/>
  <updated>2018-03-07T17:31:23.636Z</updated>
  <id>https://liquidpie.github.io/</id>
  
  <author>
    <name>Vivek Jaiswal</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Java Reference Types</title>
    <link href="https://liquidpie.github.io/2018/Java-Reference-Types/"/>
    <id>https://liquidpie.github.io/2018/Java-Reference-Types/</id>
    <published>2018-03-07T17:31:23.000Z</published>
    <updated>2018-03-07T17:31:23.636Z</updated>
    
    <content type="html"><![CDATA[<p>Do you know that Java not only have one reference type but actually four of them?</p><p>Many developers are not aware of these types and what’s the advantage of having them. I’m not sure why this concept is not evident among us, since, these types are available since Java 1.2 release.</p><p>Let’s deep dive into reference types to understand their usage.<br><a id="more"></a></p><h2 id="Why-do-we-need-so-many-different-types"><a href="#Why-do-we-need-so-many-different-types" class="headerlink" title="Why do we need so many different types?"></a>Why do we need so many different types?</h2><p>Consider I’ve a table (such as user details) that contains data which is frequently accessed. I understand a bit about performance, so instead of dipping into database every time to fetch data from this table. I’m going to take the route of <em>Cache</em>. To simplify things, let’s go ahead with an in-memory cache like a <em>Map</em>.</p><p>Whenever application needs data for a particular user, it will check if data is available in <em>Cache</em>, otherwise it will retrieve the data from database, update the <em>Cache</em> with this new data and will return data from the <em>Cache</em>.</p><p>Now the question arises ‘Is it improving the performance?’ Short answer is It Depends. If the table has few records only, it will work like charm. But, if the table holds large number of entries, it will cause <em>Cache</em> to increase linearly with the request. As, I stated earlier I know a bit about performance. I will cap the size of <em>Cache</em>, so that when max size is reached, oldest entries from <em>Cache</em> are evicted.</p><p>Well, <em>Cache</em> size cap solves the problem partially only. What if, there are some entries in <em>Cache</em>, which are rarely used later on. These entries will eat up memory unnecessarily. So, the big question is ‘How should we delete these entries sitting in <em>Cache</em> which are not being used later on?’.</p><p>As you have guessed, the answer is <strong>Reference Types</strong>.</p><h2 id="I-can’t-wait-to-know-more-about-the-Reference-Types"><a href="#I-can’t-wait-to-know-more-about-the-Reference-Types" class="headerlink" title="I can’t wait to know more about the Reference Types"></a>I can’t wait to know more about the Reference Types</h2><p><code>java.lang.ref</code> package contains the below four reference types:</p><ol><li>Strong Reference</li><li>Weak Reference</li><li>Soft Reference</li><li>Phantom Reference</li></ol><h4 id="Strong-Reference"><a href="#Strong-Reference" class="headerlink" title="Strong Reference"></a>Strong Reference</h4><p>You can see Strong Reference everywhere in Java. We create an object and then assign it to a reference.<br>Strong Reference is never <em>Garbage Collected</em>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br></pre></td></tr></table></figure><p>In the above snippet, <code>sb</code> is the Strong Reference to <code>StringBuilder</code> class object.</p><h4 id="Weak-Reference"><a href="#Weak-Reference" class="headerlink" title="Weak Reference"></a>Weak Reference</h4><p>A Weak Reference, simply put, is a reference that isn’t strong enough to force an object to remain in memory. This type of reference is eligible for <em>Garbage Collection</em>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">WeakReference&lt;StringBuilder&gt; sbWeakRef = <span class="keyword">new</span> WeakReference&lt;&gt;(sb);</span><br><span class="line"></span><br><span class="line">sb = <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure><p>In the above snippet, <code>sbWeakRef</code> is the Weak Reference to <code>StringBuilder</code> class object. After nullifying the strong reference in the third line, the object immediately becomes eligible for GC.</p><p>Well there are two different levels of weakness: <em>Soft</em> and <em>Phantom</em>.</p><h4 id="Soft-Reference"><a href="#Soft-Reference" class="headerlink" title="Soft Reference"></a>Soft Reference</h4><p>A Soft Reference is basically a weak Reference Object that remains in memory as long as possible. _GC_ reclaims this memory only when it sees that there is no more memory to allocate for new objects and it is on brink of throwing <code>OutOfMemoryError</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">SoftReference&lt;StringBuilder&gt; sbSoftRef = <span class="keyword">new</span> SoftReference&lt;&gt;(sb);</span><br><span class="line"></span><br><span class="line">sb = <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure><p>In this phase, we can still retrieve a strong reference to the object by calling the get method of the SoftReference object, which returns null if the object is already collected:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sb = sbSoftRef.get();</span><br></pre></td></tr></table></figure><h4 id="Phantom-Reference"><a href="#Phantom-Reference" class="headerlink" title="Phantom Reference"></a>Phantom Reference</h4><p>A Phantom Reference Object is useful only to know exactly when an object has been effectively removed from memory: normally they are used to fix weird <code>finalize()</code> <em>revival/resurrection</em> behavior, since they actually do not return the object itself but only help in keeping track of their memory presence.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"></span><br><span class="line">ReferenceQueue&lt;StringBuilder&gt; refQ = <span class="keyword">new</span> ReferenceQueue&lt;&gt;();</span><br><span class="line"></span><br><span class="line">PhantomReference&lt;StringBuilder&gt; sbPhantomRef = <span class="keyword">new</span> PhantomReference&lt;&gt;(sb, refQ);</span><br><span class="line"></span><br><span class="line">sb = <span class="keyword">null</span>;</span><br></pre></td></tr></table></figure><p>That’s it for now related to the <em>Reference Types</em> in Java. For <em>Concurrency</em> related problems, check my <a href="https://github.com/liquidpie/concurrency-java" target="_blank" rel="noopener">Github</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Do you know that Java not only have one reference type but actually four of them?&lt;/p&gt;
&lt;p&gt;Many developers are not aware of these types and what’s the advantage of having them. I’m not sure why this concept is not evident among us, since, these types are available since Java 1.2 release.&lt;/p&gt;
&lt;p&gt;Let’s deep dive into reference types to understand their usage.&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Click Through Rate Prediction Model</title>
    <link href="https://liquidpie.github.io/2018/Click-Through-Rate-Prediction-Model/"/>
    <id>https://liquidpie.github.io/2018/Click-Through-Rate-Prediction-Model/</id>
    <published>2018-03-07T17:30:27.000Z</published>
    <updated>2018-03-07T17:30:57.421Z</updated>
    
    <content type="html"><![CDATA[<p>Online advertising plays an important role in supporting various websites and mobile applications. Most of the major corporation in the industry today earn a major part of their revenue from advertising. Even small companies are largely supported by ads. <a href="https://en.wikipedia.org/wiki/Click-through_rate" target="_blank" rel="noopener">Click Through Rate (CTR)</a> is the ratio of users who click on a desired link to the total number of users who view the ad. In online advertising, the advertising company is usually paid only if a user clicks on the displayed advertisement. It is thus important to maximize the chances of the ad being clicked.<br><a id="more"></a></p><p>The motivation of this project comes from a Kaggle competition called <strong>Display Advertising Challenge</strong> of 2014, where the goal was to predict whether an ad will be clicked, based on the traffic logs.</p><p>Today, CTR estimation models are generally linear, ranging from logistic regression and Naive Bayes to FTRL logistic regression and Bayesian probit regression. All of these are based on sparse features with one-hot encoding. The advantage of linear model is they are easy to implement and provides efficient model training. The disadvantage of these linear model is their low performance and inadequacy to work with non-trivial patterns. On the other side, non-linear model can work with different combinations of features and thus predict more accurately. Gradient boosting through decision trees learn how to deal with different combinational features. The problem with these models is their inefficiency to work with complex and massive data.</p><p>Deep learning has emerged as an efficient technique in computer vision, speech classification and natural language processing. Visual and textual data is said to have a spatially and temporally related, such deep structure can be used to resolve local dependency and establish a dense representation of the feature space. With this type of learning, Deep learning can also be a possible good option to work on CTR ad prediction. In our implementation, we used both the machine learning techniques to train the model and make target predictions.</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>The dataset used for this project is publicly available and provided by <a href="http://labs.criteo.com/" target="_blank" rel="noopener">Criteo Labs</a>. The raw data consists of two text files one each for training and testing. The training file consists of approximately 45 million records. The test file consists of approximately 6 million records.</p><p>The training file has records on each newline. Each line consists of tab separated entities. These entities represent the features for each record. Every record is a set of features and a label. There are a total of 39 features. The first 13 features are continuous features and the rest are categorical. The first value in each new record is a label. All the data is anonymized and the dataset does not provide details about what each feature represents. Although we assume that some of these features represent the information about the advertisement being displayed, some might have information about the user to whom the advertisement is being displayed.</p><p>One important thing to note about the dataset is that each feature contains large number of missing values. This was expected because many times the advertisement display might not have all the information about the user. Sometimes the feature might not be applicable to the user or the advertisement being displayed. We have taken care to fill in these missing values with appropriate filler information as described later in the report.</p><p>All this information also applies to the test dataset except that it does not have labels.</p><h2 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h2><p>Challenges of CTR prediction requires special treatment to machine learning techniques and good feature engineering and selection for this huge dataset. Our approach in general is to apply three machine learning algorithms for CTR prediction: TensorFlow’s <code>Wide and Deep learning</code>, <code>Random Forest</code> and <code>Gradient Boosted Decision Trees</code>. Separate feature engineering techniques were applied for different algorithms. Finally, the accuracy was compared.</p><p>Following sections provide detailed description of each of the algorithm’s implementation and how we increased the accuracy.</p><h3 id="Wide-and-Deep-Learning"><a href="#Wide-and-Deep-Learning" class="headerlink" title="Wide and Deep Learning"></a>Wide and Deep Learning</h3><p>Wide and Deep Learning is an concept introduced by Google and it can be implemented using <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn" target="_blank" rel="noopener">TF.contrib.learn</a> library. Wide and Deep Learning aims to achieve both, <em>memorization</em> and <em>generalization</em>.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Memorization can be defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data.</span><br><span class="line">Generalization, on the other hand, is based on transitivity of correlation and explores new features combinations that have never or rarely occurred in the past.</span><br></pre></td></tr></table></figure></p><p>The Wide and Deep learning framework jointly train feed-forward neural networks with embedding and linear model. The Wide component include all the cross-product transformation of different categorical features available. For the Deep part of the model, a 8-dimensional embedding vector is learned for each categorical feature. All the embedding are concatenated with dense (continuous features). The concatenated vector is then fed into ReLU layers, and finally logistic output unit. The wide component and deep component are combined using a weighted sum of their output log odds as the prediction, which is fed to one common logistic loss function for joint training.</p><img src="https://github.com/liquidpie/Click-Through-Rate-Prediction-Ad-Display/raw/master/resources/classifiers.png" width="796" height="201" title="Wide and Deep Learning Model"><h4 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h4><p>The data was pre-processed using Pandas library available for Python. First, the tab separated file was read into Panda’s dataframe in batches. The primary purpose was to fill all the missing values in the dataset. For continuous feature columns, the missing data in a particular column was filled using median and for categorical columns, mode was used. We also calculated the number of unique features in all the categorical columns which further helped in feature engineering.</p><h4 id="Building-the-Model"><a href="#Building-the-Model" class="headerlink" title="Building the Model"></a>Building the Model</h4><p>The next step after preprocessing the data was to build the model using <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn" target="_blank" rel="noopener">TF.contrib.learn</a> API. An important decision to make was to decide what kind of data goes into wide and deep parts of the model. Three feature transformation techniques were used before feeding the data into the model. First, all the categorical feature column data was hashed to convert the format of data from string to numerical. The bucket size for hashing was decided according to the number of unique features in the column. Second was to generate new cross-correlated columns that took care of all the feature combinations that were rarely or never seen. The last and the most important feature transformation was to convert the categorical feature columns into embedding columns. The embedding aims to reduce the sparse feature columns into dense. The wide part of the model was trained using cross-correlated and categorical feature column, while the deep part was trained on continuous feature and embedding columns. The deep part data if fed into three hidden ReLU layer.</p><h3 id="Random-Forests-Implementation"><a href="#Random-Forests-Implementation" class="headerlink" title="Random Forests Implementation"></a>Random Forests Implementation</h3><p>Initially when we began working on the project, we started out with implementing a Logistic Regression model that could provide us with a numerical estimate of the probability that an advertisement will be clicked by a user. The initial accuracy of logistic regression was not high and we realized that this might not be a best approach to solve this problem. We then decided to try Random Forests.</p><p>The following sub-sections describe the data processing, feature engineering, feature selection and model design of the Random Forests implementation for this work.</p><h4 id="Data-Preprocessing-1"><a href="#Data-Preprocessing-1" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h4><p>We first used the tab separated data and created a Spark RDD. We then created a new Spark DataFrame from this RDD. The initial DataFrame had all columns set to contain string data.</p><h4 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h4><p>Our feature engineering mainly revolves around filling the missing values with appropriate data. Since we do not have information about the details contained in each of the columns, we decided that we need to go for a statistics based approach which will remain the same across all the features of the same type. That is, all missing values in the continuous features will have the same value regardless of what the column might represent. Same goes for the categorical features.</p><p>We replaced the missing values in continuous features with the median of all the values of that column. Spark allows us to calculate the median of a column in a DataFrame and also to replace the missing values with the computed median.</p><p>For categorical features we first decided to replace the missing values with the most frequent value of a column. When this implementation was executed, we realized that some columns had to many unique values and finding one value with highest frequency is consuming a lot of computing time. We thus decided to replace the missing values with the hashed representation of the word “unknown”. Consider a column that might be holding the URLs of the website that the advertisement might be displayed on. This column can have a lot of unique values as even a slight change in the URL essentially makes it a new value. In this case, it does not make much sense to try to compute the one value with highest frequency of occurrence as there might be multiple values with the same frequency.</p><h4 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h4><p>After our initial training attempt on the large dataset using all features, we realized that the Random Forests model was not able to train efficiently. This might be due to the fact that the way Random Forests work internally might not work well with features having large number of unique values. We thus decided to use feature selection to reduce the number of columns and hence obtain a feature set that best described the data present.</p><p>For this purpose, we used the Chi Squared Selector as implemented in Apache Spark framework. We implemented the Chi Squared Selector to work only on the categorical features. The reason is the fact that categorical features are likely to have a lot of values which are repeated multiple times in a given column. But the same might not be true for the continuous features. Since Chi Squared Selector depends on the values in the columns, it makes sense that the selector be applied only to the categorical features.</p><p>The only challenge that we ran into while implementing the Chi Squared Selector is the limitation it imposes over the number of unique values in a column. The restriction imposed by Chi Squared Selector is a maximum of 10,000 unique values in a column. We had some columns which exceeded this limitation, we thus decided to skip them from feature selection but kept them in the resulting DataFrame.</p><h4 id="Random-Forest-Model"><a href="#Random-Forest-Model" class="headerlink" title="Random Forest Model"></a>Random Forest Model</h4><p>Our Implementation of Random Forests is as follows, the model has a maximum tree depth of 8, good accuracy was obtained when we kept the maximum number of trees to 128. The impurity was set to use the gini impurity. One important thing to note is the number of max bins. This number was set to 2400000. This was again caused by the fact that a lot of values is some column were unique.</p><h4 id="Gradient-Boosted-Trees"><a href="#Gradient-Boosted-Trees" class="headerlink" title="Gradient Boosted Trees"></a>Gradient Boosted Trees</h4><p>To improve the accuracy on the test set, we implemented Gradient Boosted Trees (GBTs). We used the GBT implementation of Apache Spark framework. According to the documentation of Apache Spark, GBTs are idle for binary classification and works well with both categorical and numerical features. The GBT model is designed to use a max iteration of 100. The results section will discuss more about the accuracy obtained by using Gradient Boosted Trees.</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>The results after evaluating the different models are below:</p><h4 id="Random-Forest-GBDT-Classifier"><a href="#Random-Forest-GBDT-Classifier" class="headerlink" title="Random Forest/GBDT Classifier"></a>Random Forest/GBDT Classifier</h4><p>To evaluate the performance and predicting the target labels(Click-1/Non-Clicked-0) the model is trained with following specification:</p><p><strong>Machine Specification:</strong></p><table><thead><tr><th style="text-align:left">Specification</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Amazon EC2 Cluster</td><td style="text-align:left">m4.2xlarge machine (1 master &amp; 3 slaves)</td></tr><tr><td style="text-align:left">Machine Memory</td><td style="text-align:left">87.2 GB</td></tr><tr><td style="text-align:left">File System</td><td style="text-align:left">Apache Hadoop</td></tr></tbody></table><p><strong>Random Forest Hyperparameter:</strong></p><table><thead><tr><th style="text-align:left">Specification</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Tree Depth</td><td style="text-align:left">16</td></tr><tr><td style="text-align:left">MaxBin</td><td style="text-align:left">2400000</td></tr><tr><td style="text-align:left">Number of Tree</td><td style="text-align:left">128</td></tr></tbody></table><p><strong>GBDT Classifier builds the model with below hypermeter:</strong></p><table><thead><tr><th style="text-align:left">Specification</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Max Iteration</td><td style="text-align:left">100</td></tr></tbody></table><h4 id="Wide-and-Deep-Tensorflow"><a href="#Wide-and-Deep-Tensorflow" class="headerlink" title="Wide and Deep Tensorflow"></a>Wide and Deep Tensorflow</h4><p><strong>Wide and Deep Model builds on single instance with below specification:</strong></p><table><thead><tr><th style="text-align:left">Specification</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Amazon EC2 instance</td><td style="text-align:left">m4.4xlarge</td></tr><tr><td style="text-align:left">Machine Memory</td><td style="text-align:left">64Gb</td></tr></tbody></table><p><strong>Following are model specification:</strong></p><table><thead><tr><th style="text-align:left">Specification</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">Wide Columns</td><td style="text-align:left">C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19,C20,C21,C22,C23,C24,C25,C26</td></tr><tr><td style="text-align:left">Deep Column</td><td style="text-align:left">I1,I2,I3,I4,I5,I6,I7,I8,I9,I10,I11,I12,I13    C1,C2,C3,C4,C5,C6,C7,C8,C9,C10,C11,C12,C13,C14,C15,C16,C17,C18,C19,C20,C21,C22,C23,C24,C25,C26</td></tr><tr><td style="text-align:left">Hidden Layers</td><td style="text-align:left">[512,256,128,64]</td></tr><tr><td style="text-align:left">DNN Optimizer</td><td style="text-align:left">AdaGrad</td></tr><tr><td style="text-align:left">Linear Model Optimizer</td><td style="text-align:left">SGD</td></tr></tbody></table><h4 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h4><img src="https://github.com/liquidpie/Click-Through-Rate-Prediction-Ad-Display/raw/master/resources/accuracy.png" width="717" height="431" title="Accuracy Graph"><blockquote><p>This article is an excerpt from research done by my friend <strong>Dharamendra Kumar</strong> and is posted with his permission.<br>To know more about this, check project’s <a href="https://github.com/dk67604/Click-Through-Rate-Prediction-Ad-Display" target="_blank" rel="noopener">Github</a></p><footer><strong>Vivek Jaiswal</strong></footer></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Online advertising plays an important role in supporting various websites and mobile applications. Most of the major corporation in the industry today earn a major part of their revenue from advertising. Even small companies are largely supported by ads. &lt;a href=&quot;https://en.wikipedia.org/wiki/Click-through_rate&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Click Through Rate (CTR)&lt;/a&gt; is the ratio of users who click on a desired link to the total number of users who view the ad. In online advertising, the advertising company is usually paid only if a user clicks on the displayed advertisement. It is thus important to maximize the chances of the ad being clicked.&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
